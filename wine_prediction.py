# -*- coding: utf-8 -*-
"""Wine Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nBkQZLPH-ttunvwxlyRUp0bu3uFL0MrE
"""

# Import dependencies 

import numpy as np
import pandas as pd
import matplotlib as plt
import plotly.express as px
import seaborn as sns

# Read CSV file

df = pd.read_csv("/content/Data/winequality-red (1).csv")

# Look at dataframe

df.head()

# Check for duplicates

print(df.isna().sum())

# Count & Quality

fig = px.histogram(df,x='quality')
fig.show()

# Create correlation matrix

corr = df.corr()
plt.pyplot.subplots(figsize=(15,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))

# Create Classification version of target variable

df['goodquality'] = [1 if x >= 7 else 0 for x in df['quality']]

# Separate feature variables and target variable

X = df.drop(['quality','goodquality'], axis = 1)
y = df['goodquality']

# See proportion of good vs bad wines

df['goodquality'].value_counts()

# Normalize feature variables

from sklearn.preprocessing import StandardScaler
X_features = X
X = StandardScaler().fit_transform(X)

# Split the data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)

# Decision Tree Classifier

from sklearn.metrics import classification_report
from sklearn.tree import DecisionTreeClassifier
model1 = DecisionTreeClassifier(random_state=1)
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)
print(classification_report(y_test, y_pred1))

# Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(random_state=1)
model2.fit(X_train, y_train)
y_pred2 = model2.predict(X_test)
print(classification_report(y_test, y_pred2))

# AdaBoost Classifier

from sklearn.ensemble import AdaBoostClassifier
model3 = AdaBoostClassifier(random_state=1)
model3.fit(X_train, y_train)
y_pred3 = model3.predict(X_test)
print(classification_report(y_test, y_pred3))

# Gradient Boost Classifier

from sklearn.ensemble import GradientBoostingClassifier
model4 = GradientBoostingClassifier(random_state=1)
model4.fit(X_train, y_train)
y_pred4 = model4.predict(X_test)
print(classification_report(y_test, y_pred4))

# XGB Classifier

import xgboost as xgb
model5 = xgb.XGBClassifier(random_state=1)
model5.fit(X_train, y_train)
y_pred5 = model5.predict(X_test)
print(classification_report(y_test, y_pred5))

# Histograms

df.hist(bins=10, figsize=(20,20))
plt.pyplot.show()

# Filtering df for only good quality

df_temp = df[df['goodquality']==1]
df_temp.describe()

# Filtering df for only bad quality

df_temp2 = df[df['goodquality']==0]
df_temp2.describe()

"""Good quality wines have higher levels of alcohol on average, have a lower volatile acidity on average, higher levels of sulphates on average, and higher levels of residual sugar on average."""